# ORION (Optimized Reasoning & Intelligence On Node)

**Next-Gen Heterogeneous Edge AI System: Integrating Real-time Vision & LLM Reasoning on RK3588**

[![Platform](https://img.shields.io/badge/Platform-Rockchip%20RK3588-blue?logo=linux&logoColor=white)](https://www.rock-chips.com/)
[![OS](https://img.shields.io/badge/OS-Embedded%20Linux%20(5.10%20LTS)-green?logo=tux&logoColor=white)](https://kernel.org)
[![Framework](https://img.shields.io/badge/Framework-RKNN%20%7C%20MPP%20%7C%20RGA-orange)](https://github.com/airockchip/rknn-toolkit2)
[![License](https://img.shields.io/badge/License-Apache%202.0-lightgrey)](LICENSE)

## üìñ Introduction

**ORION** is a high-performance, heterogeneous edge AI framework engineered specifically for the Rockchip RK3588 platform. It transcends traditional "vision-only" edge systems by fusing **Real-time Object Detection (YOLO)** with **Semantic Reasoning (DeepSeek LLM)** into a unified, zero-copy pipeline.

Current edge AI solutions often suffer from memory bottlenecks and serialized processing. ORION solves this by implementing a **DMA-BUF (DRM) Zero-Copy architecture**, allowing the CPU, NPU, and RGA to share memory without redundant copying. The result is a system capable of **60+ FPS visual perception** and **14 tokens/s complex logic reasoning** simultaneously, enabling true "Embedded AGI" capabilities on <10W power consumption.

## üöÄ Key Features

### 1. Optimized Linux BSP Construction
*   **Custom BSP Stack**: Built from Rockchip SDK with tailored U-Boot, Kernel (5.10 LTS), and minimal RootFS.
*   **Hardware-Specific DTS**: Precise device tree overlays for MIPI-CSI cameras, 3-core NPU partitioning, and PCIe accelerators.
*   **System Hardening**: Integrated **PREEMPT_RT** patch for deterministic latency; memory map optimization reduces boot time by 40%.

### 2. Zero-Copy Heterogeneous Pipeline
*   **MPP Hardware Acceleration**: 8-channel 1080p@30fps H.264/H.265 decoding with near-zero CPU usage.
*   **RGA Preprocessing**: Hardware-accelerated color space conversion (NV12‚ÜíRGB) and resizing at 1200MPix/s throughput.
*   **DRM Zero-Copy Mechanism**: Direct memory mapping between VPU (Decoder), RGA (Pre-proc), and NPU (Inference), eliminating **~2.8GB/s** of redundant memory copy operations.

### 3. Dual-Path Intelligence Architecture (Vision + Logic)
*   **Path A: Real-time Vision (YOLOv11)**
    *   Deploying state-of-the-art YOLOv11 on NPU with operator fusion and quantization calibration.
    *   Generates structured JSON perception data at **60+ FPS**.
    *   Implements "Ping-Pong" buffering strategies to maximize NPU throughput.

*   **Path B: Semantic Reasoning (DeepSeek R1 1.5B)**
    *   Fully offline LLM deployment via RKLLM with **W4A16 quantization**.
    *   Acts as the system's "Prefrontal Cortex," analyzing vision-generated JSON to make context-aware decisions.
    *   Achieves **14 tokens/s** generation speed through asynchronous pipeline optimizations.

### 4. Advanced Resource Management
*   **Asynchronous Execution Engine**: Completely decouples the high-frequency vision loop from the lower-frequency reasoning loop.
*   **3-Core NPU Saturation**: Custom thread pool manager dynamically assigns NPU Core 0/1 to Vision and Core 2 to LLM, achieving **98% NPU utilization**.
*   **Adaptive Memory Governor**: Dynamic context pruning for the LLM and aggressive buffer recycling for vision streams to fit within 8GB/16GB RAM constraints.

## üèó Architecture

```mermaid
graph TD
    subgraph "ORION Heterogeneous Architecture"
        direction TB
        
        Cam[Camera Streams] -->|"Raw Video Frames"| V1
        
        subgraph "Fast Loop: Visual Perception"
            V1["Stage 1: MPP Decoder<br/>8-ch 1080p@30fps"] 
            V2["Stage 2: RGA Preprocessor<br/>NV12‚ÜíRGB (Zero-Copy)"]
            V3["Stage 3: YOLOv11 NPU Inference<br/>60+ FPS Detection"]
            V4["Stage 4: JSON Generator<br/>Structured Scene Graph"]
            
            V1 --> V2
            V2 --> V3
            V3 --> V4
        end
        
        V4 -.->|"Scene Data Snapshot<br/>(Async Trigger)"| R1
        
        subgraph "Slow Loop: Cognitive Reasoning"
            R1["Stage 1: Context Window<br/>Shared DRAM Buffer"]
            R2["Stage 2: DeepSeek R1 1.5B<br/>W4A16 Quantized LLM"]
            R3["Stage 3: Decision Engine<br/>Logic & Safety Checks"]
            
            R1 --> R2
            R2 --> R3
        end
        
        R3 -->|"High-Level Commands"| App[Application Layer]
        V4 -->|"Real-time Overlay"| App
    end

    %% Visual Styling
    style V1 fill:#E6F7FF,stroke:#1890FF,stroke-width:2px
    style V2 fill:#E6F7FF,stroke:#1890FF,stroke-width:2px
    style V3 fill:#FFF7E6,stroke:#FA8C16,stroke-width:2px
    style V4 fill:#E6F7FF,stroke:#1890FF,stroke-width:2px
    
    %% Reasoning Styling
    style R1 fill:#F6FFED,stroke:#52C41A,stroke-width:2px
    style R2 fill:#FFF7E6,stroke:#FA8C16,stroke-width:2px
    style R3 fill:#F6FFED,stroke:#52C41A,stroke-width:2px
    
    %% External
    style Cam fill:#F0F0F0,stroke:#595959
    style App fill:#F0F0F0,stroke:#595959
```

## üìä Performance Benchmarks

| Component | Configuration | Throughput | Resource Usage | Efficiency Gain |
| :--- | :--- | :--- | :--- | :--- |
| **Vision Pipeline** | Baseline (Sequential) | 30 FPS | CPU: 78%, NPU: 45% | - |
| **Vision Pipeline** | **ORION (Async + Zero-Copy)** | **62 FPS** | **CPU: 35%**, NPU: 98% | **2.0√ó Speed, 0.5√ó CPU** |
| **LLM Inference** | Single-thread | 6 tokens/s | NPU: 60% load | - |
| **LLM Inference** | **ORION (3-Core Parallel)** | **14 tokens/s** | NPU: 99% load | **2.3√ó Speed** |
| **Full System** | Traditional Pipeline | 18 FPS + 5 t/s | Temp: 78¬∞C | - |
| **Full System** | **ORION Dual-Path** | **60 FPS + 14 t/s** | **Temp: 65¬∞C** | **3.3√ó Overall Perf** |

## üõ†Ô∏è Build & Usage

### Prerequisites
*   **Hardware**: Rockchip RK3588/RK3588S (8GB+ RAM required for LLM)
*   **Host**: Ubuntu 22.04 LTS (Docker required for cross-compilation)
*   **SDK**: Rockchip Linux SDK 5.10 + RKNN Toolkit2 v2.0+

### 1. System Setup
```bash
# Clone repository with submodules
git clone --recursive https://github.com/WNPPP0114/ORION.git
cd ORION

# Initialize cross-compilation environment
docker build -t orion-builder -f docker/Dockerfile .
docker run -v $(pwd):/workspace -it orion-builder
```

### 2. Build BSP & Firmware
```bash
# Build custom U-Boot, Kernel, and RootFS
cd bsp
./configure --platform=rk3588 --board=itop-3588
./build.sh full_image
```

### 3. Compile Core Engine
```bash
cd src
mkdir build && cd build
cmake -DCMAKE_TOOLCHAIN_FILE=../toolchain/rk3588_linux.cmake -DCMAKE_BUILD_TYPE=Release ..
make -j$(nproc)
```

### 4. Deploy Models & Run
```bash
# Transfer artifacts to board
scp ./orion_core user@rk3588:/usr/local/bin/
scp -r ../models user@rk3588:/opt/orion/

# Run the ORION Daemon on device
sudo orion_core \
  --vision_model /opt/orion/models/yolov11s.rknn \
  --llm_model /opt/orion/models/deepseek-r1-1.5b_w4a16.rknn \
  --enable_zero_copy true \
  --npu_split_mode 2:1  # 2 cores for Vision, 1 core for LLM
```

## üìÇ Project Structure
```text
ORION/
‚îú‚îÄ‚îÄ bsp/                    # Board Support Package (Kernel/U-Boot/DTS)
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ core/               # Scheduler & Thread Pool Manager
‚îÇ   ‚îú‚îÄ‚îÄ vision/             # MPP Decoder, RGA Processing, YOLO Post-process
‚îÇ   ‚îú‚îÄ‚îÄ reasoning/          # RKLLM Inference Engine & Context Manager
‚îÇ   ‚îî‚îÄ‚îÄ hal/                # DRM/DMA-BUF Hardware Abstraction Layer
‚îú‚îÄ‚îÄ models/                 # Model Conversion Scripts (ONNX -> RKNN)
‚îú‚îÄ‚îÄ tools/                  # Performance Profiling & Debug Tools
‚îî‚îÄ‚îÄ docs/                   # Architecture & API References
```

## ü§ù Contribution
Contributions are welcome! Please see our [Contribution Guidelines](CONTRIBUTING.md). We are particularly interested in:
- Extending support to RK3568 / RK3576 platforms.
- Implementing support for Multi-modal LLMs (LlaVA).
- Optimizing RGA memory alignment strategies.

## ‚ú® Acknowledgements
- **Rockchip**: For the RKNN and MPP frameworks.
- **DeepSeek**: For the open-weights reasoning models.
- **Linux Kernel Community**: For the PREEMPT_RT patchset.

---
**Project Duration**: June 2025 - June 2026
**Hardware Platform**: Rockchip RK3588 (8GB RAM)
**GitHub**: https://github.com/WNPPP0114/ORION
